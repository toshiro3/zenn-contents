---
title: "Spark Structured Streaming ã§ Kafka ã‹ã‚‰ Iceberg ã¸ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ›¸ãè¾¼ã¿"
emoji: "ğŸŒŠ"
type: "tech"
topics: ["spark", "kafka", "iceberg", "streaming", "python"]
published: true
---

## ã¯ã˜ã‚ã«

ã“ã®è¨˜äº‹ã§ã¯ã€Spark Structured Streaming ã‚’ä½¿ã£ã¦ Kafka ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€Apache Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ›¸ãè¾¼ã‚€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

## å‰ææ¡ä»¶

å‰å›ã®è¨˜äº‹ã§æ§‹ç¯‰ã—ãŸ Docker Compose ç’°å¢ƒã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

https://zenn.dev/toshiro3/articles/spark-kafka-iceberg-docker-setup

ã“ã®ç’°å¢ƒã«ã¯ä»¥ä¸‹ã®ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒ `spark-defaults.conf` ã§è¨­å®šæ¸ˆã¿ã§ã™ï¼š

```conf
spark.jars.packages  org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1
```

- **spark-sql-kafka**: Kafka ã‹ã‚‰ã®èª­ã¿æ›¸ãã«å¿…è¦
- **iceberg-spark-runtime**: `tabulario/spark-iceberg` ã‚¤ãƒ¡ãƒ¼ã‚¸ã«åŒæ¢±æ¸ˆã¿

## Structured Streaming ã¨ã¯

Spark Structured Streaming ã¯ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ DataFrame/SQL ã§æ‰±ãˆã‚‹ API ã§ã™ã€‚

### ç‰¹å¾´

- **ãƒãƒƒãƒã¨åŒã˜ API**: `spark.read` â†’ `spark.readStream` ã«å¤‰ãˆã‚‹ã ã‘
- **ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒå‡¦ç†**: çŸ­ã„é–“éš”ã§ãƒãƒƒãƒå‡¦ç†ã‚’ç¹°ã‚Šè¿”ã™
- **Exactly-once ä¿è¨¼**: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã‚ˆã‚Šé‡è¤‡ãªã—ãƒ»æ¬ æãªã—ã‚’å®Ÿç¾
- **Iceberg ã¨ã®çµ±åˆ**: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ç›´æ¥ Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã¿å¯èƒ½

### ãƒãƒƒãƒ vs ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°

```python
# ãƒãƒƒãƒèª­ã¿è¾¼ã¿
df = spark.read.format("kafka").option(...).load()

# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èª­ã¿è¾¼ã¿
df = spark.readStream.format("kafka").option(...).load()
```

## å®Ÿè£…ï¼šKafka â†’ Iceberg ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

EC ã‚µã‚¤ãƒˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡Œå‹•ãƒ­ã‚°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å‡¦ç†ã™ã‚‹ã‚·ãƒŠãƒªã‚ªã§å®Ÿè£…ã—ã¾ã™ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Kafka     â”‚    â”‚ Spark Structured â”‚    â”‚    Iceberg      â”‚
â”‚   events    â”‚â”€â”€â”€â–¶â”‚    Streaming     â”‚â”€â”€â”€â–¶â”‚   user_events   â”‚
â”‚   topic     â”‚    â”‚   (JSONå¤‰æ›)      â”‚    â”‚     table       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Checkpoint    â”‚
                   â”‚   (é€²æ—ç®¡ç†)     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```json
{
  "event_id": "evt_0001",
  "user_id": "user_001",
  "event_type": "page_view",
  "page": "/products/123",
  "timestamp": "2024-12-31T10:00:00Z"
}
```

### Step 1: Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Namespace ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ
spark.sql("CREATE NAMESPACE IF NOT EXISTS demo.streaming")

spark.sql("""
    CREATE TABLE IF NOT EXISTS demo.streaming.user_events (
        event_id STRING,
        user_id STRING,
        event_type STRING,
        page STRING,
        event_time TIMESTAMP
    ) USING iceberg
""")
```

### Step 2: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ Kafka ã«é€ä¿¡

```python
import json
from datetime import datetime, timedelta

events = []
base_time = datetime.now()

for i in range(10):
    event = {
        "event_id": f"evt_{i:04d}",
        "user_id": f"user_{i % 3:03d}",
        "event_type": ["page_view", "click", "purchase"][i % 3],
        "page": f"/products/{100 + i}",
        "timestamp": (base_time + timedelta(seconds=i)).isoformat()
    }
    events.append((event["event_id"], json.dumps(event)))

df_events = spark.createDataFrame(events, ["key", "value"])

df_events.write \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("topic", "events") \
    .save()
```

### Step 3: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†

```python
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType

# JSON ã‚¹ã‚­ãƒ¼ãƒå®šç¾©
schema = StructType([
    StructField("event_id", StringType()),
    StructField("user_id", StringType()),
    StructField("event_type", StringType()),
    StructField("page", StringType()),
    StructField("timestamp", StringType())  # ä¸€æ—¦ String ã§å—ã‘å–ã‚‹
])

# Kafka ã‹ã‚‰ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èª­ã¿è¾¼ã¿
streaming_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "events") \
    .option("startingOffsets", "earliest") \
    .load()

# JSON ãƒ‘ãƒ¼ã‚¹ã¨å¤‰æ›
parsed_df = streaming_df \
    .select(from_json(col("value").cast("string"), schema).alias("data")) \
    .select(
        col("data.event_id"),
        col("data.user_id"),
        col("data.event_type"),
        col("data.page"),
        col("data.timestamp").cast("timestamp").alias("event_time")  # TIMESTAMP ã«å¤‰æ›
    )

# Iceberg ã«æ›¸ãè¾¼ã¿
query = parsed_df.writeStream \
    .format("iceberg") \
    .outputMode("append") \
    .option("checkpointLocation", "/home/iceberg/checkpoints/user_events") \
    .toTable("demo.streaming.user_events")
```

:::message
**timestamp ã®å‹å¤‰æ›ã«ã¤ã„ã¦**

JSON ã® `timestamp` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ ISO8601 å½¢å¼ã®æ–‡å­—åˆ—ï¼ˆ`2024-12-31T10:00:00Z`ï¼‰ã§ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ `StringType()` ã§å—ã‘å–ã‚Šã€å¾Œã‹ã‚‰ `.cast("timestamp")` ã§å¤‰æ›ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ä»¥ä¸‹ã®ç†ç”±ã‹ã‚‰ã§ã™ï¼š
- JSON ã‚½ãƒ¼ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒä¸å®šãªå ´åˆã«æŸ”è»Ÿã«å¯¾å¿œã§ãã‚‹
- ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒãƒƒã‚°ãŒã—ã‚„ã™ã„

`TimestampType()` ã‚’ç›´æ¥ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ãŒã€ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚„æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ä¸æ•´åˆã§ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
:::

### Step 4: çµæœç¢ºèª

```python
import time
time.sleep(5)

# ãƒ‡ãƒ¼ã‚¿ç¢ºèª
spark.sql("SELECT * FROM demo.streaming.user_events ORDER BY event_time").show()

# ä»¶æ•°ç¢ºèª
spark.sql("SELECT COUNT(*) FROM demo.streaming.user_events").show()

# ã‚¯ã‚¨ãƒªåœæ­¢
query.stop()
```

:::message alert
ç’°å¢ƒã‚„ãƒ‡ãƒ¼ã‚¿é‡ã«ã‚ˆã£ã¦ã¯ã€æ›¸ãè¾¼ã¿å®Œäº†ã¾ã§ 5 ç§’ä»¥ä¸Šã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãŒè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã¯ã€`time.sleep(10)` ã«å¤‰æ›´ã™ã‚‹ã‹ã€ä»¥ä¸‹ã®ã‚ˆã†ã« `awaitTermination` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š

```python
# æŒ‡å®šæ™‚é–“å¾…æ©Ÿï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
query.awaitTermination(timeout=10)
```
:::

## ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ

`checkpointLocation` ã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã®é€²æ—ã‚’ä¿å­˜ã™ã‚‹å ´æ‰€ã§ã™ã€‚

### å½¹å‰²

- **ã‚ªãƒ•ã‚»ãƒƒãƒˆç®¡ç†**: Kafka ã®ã©ã“ã¾ã§èª­ã‚“ã ã‹ã‚’è¨˜éŒ²
- **éšœå®³å¾©æ—§**: å†èµ·å‹•æ™‚ã«ç¶šãã‹ã‚‰å‡¦ç†ã‚’å†é–‹
- **Exactly-once ä¿è¨¼**: é‡è¤‡å‡¦ç†ã‚’é˜²æ­¢

### æ³¨æ„ç‚¹

- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯**ã‚¯ã‚¨ãƒªã”ã¨ã«ä¸€æ„ã®ãƒ‘ã‚¹**ãŒå¿…è¦
- ãƒ‘ã‚¹ã‚’å¤‰æ›´ã™ã‚‹ã¨ã€ä»¥å‰ã®é€²æ—ï¼ˆã‚ªãƒ•ã‚»ãƒƒãƒˆï¼‰ãŒå¼•ãç¶™ãŒã‚Œãš**æ–°è¦ã‚¯ã‚¨ãƒªã¨ã—ã¦æ‰±ã‚ã‚Œã‚‹**
- æœ¬ç•ªç’°å¢ƒã§ã¯ S3 ãªã©ã®æ°¸ç¶šã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ä½¿ç”¨

:::message
ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’å¤‰æ›´ã—ãŸå ´åˆã®æŒ™å‹•ã¯ `startingOffsets` ã®è¨­å®šã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ï¼š
- `earliest`: æœ€åˆã‹ã‚‰å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å†å‡¦ç†
- `latest`: èµ·å‹•å¾Œã«å…¥ã£ã¦ããŸãƒ‡ãƒ¼ã‚¿ã®ã¿å‡¦ç†ï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
:::

```python
# ä¾‹: S3 ã‚’ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ä½¿ç”¨
.option("checkpointLocation", "s3://my-bucket/checkpoints/user_events")
```

## ãƒˆãƒªã‚¬ãƒ¼è¨­å®š

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã§ãã‚‹ã ã‘æ—©ããƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚’å‡¦ç†ã—ã¾ã™ãŒã€é–“éš”ã‚’æŒ‡å®šã§ãã¾ã™ã€‚

### processingTime

æŒ‡å®šã—ãŸé–“éš”ã§ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œï¼š

```python
query = parsed_df.writeStream \
    .format("iceberg") \
    .outputMode("append") \
    .option("checkpointLocation", "/home/iceberg/checkpoints/user_events") \
    .trigger(processingTime="10 seconds") \
    .toTable("demo.streaming.user_events")
```

### availableNow

ç¾åœ¨åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚’ã™ã¹ã¦å‡¦ç†ã—ã¦åœæ­¢ï¼ˆãƒãƒƒãƒçš„ãªä½¿ã„æ–¹ï¼‰ï¼š

```python
query = parsed_df.writeStream \
    .format("iceberg") \
    .outputMode("append") \
    .option("checkpointLocation", "/home/iceberg/checkpoints/user_events") \
    .trigger(availableNow=True) \
    .toTable("demo.streaming.user_events")
```

## Iceberg ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ

ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ›¸ãè¾¼ã¿ã§ã¯ã€ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã”ã¨ã«ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒä½œæˆã•ã‚Œã¾ã™ã€‚

```python
spark.sql("""
    SELECT 
        snapshot_id,
        committed_at,
        operation,
        summary['added-records'] as added_records
    FROM demo.streaming.user_events.snapshots
    ORDER BY committed_at
""").show(truncate=False)
```

```
+--------------------+------------------------+---------+-------------+
|snapshot_id         |committed_at            |operation|added_records|
+--------------------+------------------------+---------+-------------+
|3039719484403124022 |2026-01-06 13:20:51.998 |append   |10           |
|8383822172186054235 |2026-01-06 13:25:13.752 |append   |1            |
|3248936071893229378 |2026-01-06 13:25:14.199 |append   |4            |
+--------------------+------------------------+---------+-------------+
```

:::message
é »ç¹ãªãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã¯ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å¤§é‡ã«ç”Ÿæˆã—ã¾ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯å®šæœŸçš„ãª compaction ãŒå¿…è¦ã§ã™ã€‚
:::

## ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ã€Spark Structured Streaming ã‚’ä½¿ã£ã¦ Kafka ã‹ã‚‰ Iceberg ã¸ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ›¸ãè¾¼ã¿ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚

ç¢ºèªã§ããŸã“ã¨ï¼š
- Kafka ã‹ã‚‰ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èª­ã¿è¾¼ã¿
- JSON ãƒ‘ãƒ¼ã‚¹ã¨å‹å¤‰æ›
- Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›¸ãè¾¼ã¿
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã‚ˆã‚‹å†é–‹
- ãƒˆãƒªã‚¬ãƒ¼è¨­å®šã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³

æ¬¡å›ã¯ã€ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–¢æ•°ã‚’ä½¿ã£ãŸé›†è¨ˆå‡¦ç†ï¼ˆæ™‚é–“ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã”ã¨ã®é›†è¨ˆãªã©ï¼‰ã‚’è©¦ã—ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚

## å‚è€ƒãƒªãƒ³ã‚¯

- [Spark Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Structured Streaming + Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)
- [Apache Iceberg - Spark Structured Streaming](https://iceberg.apache.org/docs/latest/spark-structured-streaming/)
