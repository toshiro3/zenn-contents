---
title: "AWS Glue Streamingã§Kinesisã‹ã‚‰Icebergã¸ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›¸ãè¾¼ã¿"
emoji: "ğŸŒŠ"
type: "tech"
topics: ["aws", "glue", "iceberg", "kinesis", "streaming"]
published: true
---

## ã¯ã˜ã‚ã«

æœ¬è¨˜äº‹ã§ã¯ã€AWS ç’°å¢ƒã§ **Kinesis + Glue Streaming + Iceberg** ã‚’ä½¿ã£ãŸã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’æ¤œè¨¼ã—ã¦ã„ãã¾ã™ã€‚

ä»¥å‰ã€ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ Spark Structured Streaming + Kafka + Iceberg ã®æ§‹æˆã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚æœ¬è¨˜äº‹ã§ã¯ã€ãã®æ§‹æˆã‚’ AWS ã®ãƒãƒãƒ¼ã‚¸ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã§å®Ÿè£…ã™ã‚‹ã¨ã©ã†ãªã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚

https://zenn.dev/toshiro3/articles/spark-kafka-iceberg-docker-setup

https://zenn.dev/toshiro3/articles/spark-kafka-iceberg-streaming-basics

## ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã¨ã®å¯¾å¿œé–¢ä¿‚

| ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ | AWS ç’°å¢ƒ |
|-------------|----------|
| Apache Kafka | Amazon Kinesis Data Streams |
| Spark Structured Streaming | AWS Glue Streamingï¼ˆSpark ãƒ™ãƒ¼ã‚¹ï¼‰ |
| Iceberg REST Catalog | AWS Glue Data Catalog |
| MinIOï¼ˆS3 äº’æ›ï¼‰ | Amazon S3 |

AWS Glue ã¯å†…éƒ¨çš„ã« Apache Spark ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€Spark Structured Streaming ã®ã‚³ãƒ¼ãƒ‰ã‚’ã»ã¼ãã®ã¾ã¾ç§»æ¤ã§ãã¾ã™ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Producer  â”‚â”€â”€â”€â–¶â”‚ Kinesis Data    â”‚â”€â”€â”€â–¶â”‚ Glue Streaming Job      â”‚
â”‚ (ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿)â”‚    â”‚ Streams         â”‚    â”‚ (Spark Structured       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  Streaming)             â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                                      â–¼
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚ S3 + Glue Data Catalog  â”‚
                                          â”‚ (Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«)        â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                                      â–¼
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚ Amazon Athena           â”‚
                                          â”‚ (ã‚¯ã‚¨ãƒª)                  â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å‰ææ¡ä»¶

- AWS CLI ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»è¨­å®šæ¸ˆã¿
- é©åˆ‡ãª IAM æ¨©é™ï¼ˆGlueã€Kinesisã€S3ã€Athenaï¼‰
- AWS ãƒªãƒ¼ã‚¸ãƒ§ãƒ³: ap-northeast-1ï¼ˆæ±äº¬ï¼‰

## ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

```bash
export AWS_REGION="ap-northeast-1"
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
export PREFIX="glue-streaming-iceberg"
export KINESIS_STREAM="${PREFIX}-stream"
export S3_BUCKET_SCRIPTS="${PREFIX}-scripts-${AWS_ACCOUNT_ID}"
export S3_BUCKET_DATA="${PREFIX}-data-${AWS_ACCOUNT_ID}"
export GLUE_DATABASE="${PREFIX//-/_}_db"
export GLUE_ROLE="${PREFIX}-glue-role"
```

## Step 1: S3 ãƒã‚±ãƒƒãƒˆã®ä½œæˆ

ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”¨ã¨ãƒ‡ãƒ¼ã‚¿ç”¨ã® 2 ã¤ã®ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”¨ãƒã‚±ãƒƒãƒˆ:

```bash
aws s3 mb s3://${S3_BUCKET_SCRIPTS} --region ${AWS_REGION}
```

ãƒ‡ãƒ¼ã‚¿ç”¨ãƒã‚±ãƒƒãƒˆï¼ˆIceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¿å­˜å…ˆï¼‰:

```bash
aws s3 mb s3://${S3_BUCKET_DATA} --region ${AWS_REGION}
```

## Step 2: Kinesis Data Streams ã®ä½œæˆ

```bash
aws kinesis create-stream \
  --stream-name ${KINESIS_STREAM} \
  --stream-mode-details StreamMode=ON_DEMAND
```

ON_DEMAND ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚·ãƒ£ãƒ¼ãƒ‰æ•°ã‚’æ°—ã«ã›ãšã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§ãã¾ã™ã€‚

:::message
**ã‚·ãƒ£ãƒ¼ãƒ‰æ•°ã¨ Glue workers ã®é–¢ä¿‚**

Glue Streaming ã¯å†…éƒ¨çš„ã« Spark ã® Direct Consumer ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€Kinesis ã®ã‚·ãƒ£ãƒ¼ãƒ‰æ•°ã«åŸºã¥ã„ã¦ä¸¦åˆ—åº¦ãŒæ±ºã¾ã‚Šã¾ã™ã€‚ãã®ãŸã‚ã€Glue å´ã® `number-of-workers` ã‚’å¢—ã‚„ã—ã¦ã‚‚ã€ã‚·ãƒ£ãƒ¼ãƒ‰æ•°ãŒå°‘ãªã„ã¨ãƒªã‚½ãƒ¼ã‚¹ãŒæœ‰åŠ¹æ´»ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯ã€æƒ³å®šã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã«å¿œã˜ã¦ã‚·ãƒ£ãƒ¼ãƒ‰æ•°ã¨ worker æ•°ã®ãƒãƒ©ãƒ³ã‚¹ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚
:::

## Step 3: Glue Database ã®ä½œæˆ

```bash
aws glue create-database \
  --database-input '{
    "Name": "'"${GLUE_DATABASE}"'",
    "Description": "Database for Glue Streaming Iceberg demo"
  }'
```

## Step 4: Lake Formation ã®è¨­å®š

AWS ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ Lake Formation ãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ã‚‹å ´åˆã€Glue Data Catalog ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã«è¿½åŠ ã®æ¨©é™è¨­å®šãŒå¿…è¦ã§ã™ã€‚

ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ Lake Formation ç®¡ç†è€…ã«è¿½åŠ :

```bash
CURRENT_USER_ARN=$(aws sts get-caller-identity --query 'Arn' --output text)
echo "Current user: ${CURRENT_USER_ARN}"

aws lakeformation put-data-lake-settings \
  --data-lake-settings '{
    "DataLakeAdmins": [
      {"DataLakePrincipalIdentifier": "'"${CURRENT_USER_ARN}"'"}
    ],
    "CreateDatabaseDefaultPermissions": [
      {
        "Principal": {"DataLakePrincipalIdentifier": "IAM_ALLOWED_PRINCIPALS"},
        "Permissions": ["ALL"]
      }
    ],
    "CreateTableDefaultPermissions": [
      {
        "Principal": {"DataLakePrincipalIdentifier": "IAM_ALLOWED_PRINCIPALS"},
        "Permissions": ["ALL"]
      }
    ]
  }'
```

Glue Database ã« IAM æ¨©é™ã‚’ä»˜ä¸:

```bash
aws lakeformation grant-permissions \
  --principal DataLakePrincipalIdentifier="IAM_ALLOWED_PRINCIPALS" \
  --resource '{"Database": {"Name": "'"${GLUE_DATABASE}"'"}}' \
  --permissions "ALL"
```

ã“ã®è¨­å®šã«ã‚ˆã‚Šã€IAM ãƒãƒªã‚·ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ãŒæœ‰åŠ¹ã«ãªã‚Šã€Glue Job ãŒä½œæˆã—ãŸãƒ†ãƒ¼ãƒ–ãƒ«ã« Athena ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

## Step 5: IAM ãƒ­ãƒ¼ãƒ«ã®ä½œæˆ

### ä¿¡é ¼ãƒãƒªã‚·ãƒ¼

```bash
cat > glue-trust-policy.json << 'EOF'
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "glue.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

aws iam create-role \
  --role-name ${GLUE_ROLE} \
  --assume-role-policy-document file://glue-trust-policy.json
```

### ãƒãƒãƒ¼ã‚¸ãƒ‰ãƒãƒªã‚·ãƒ¼ã®ã‚¢ã‚¿ãƒƒãƒ

```bash
aws iam attach-role-policy \
  --role-name ${GLUE_ROLE} \
  --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole

aws iam attach-role-policy \
  --role-name ${GLUE_ROLE} \
  --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess

aws iam attach-role-policy \
  --role-name ${GLUE_ROLE} \
  --policy-arn arn:aws:iam::aws:policy/AmazonKinesisReadOnlyAccess
```

### ã‚«ã‚¹ã‚¿ãƒ ãƒãƒªã‚·ãƒ¼

Glue Data Catalog ã¸ã® Iceberg ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿æ¨©é™ã‚’è¿½åŠ ã—ã¾ã™ã€‚

```bash
cat > glue-custom-policy.json << EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "GlueCatalogAccess",
      "Effect": "Allow",
      "Action": [
        "glue:GetDatabase",
        "glue:GetDatabases",
        "glue:CreateTable",
        "glue:GetTable",
        "glue:GetTables",
        "glue:UpdateTable",
        "glue:DeleteTable",
        "glue:GetPartition",
        "glue:GetPartitions",
        "glue:CreatePartition",
        "glue:BatchCreatePartition",
        "glue:DeletePartition"
      ],
      "Resource": [
        "arn:aws:glue:${AWS_REGION}:${AWS_ACCOUNT_ID}:catalog",
        "arn:aws:glue:${AWS_REGION}:${AWS_ACCOUNT_ID}:database/${GLUE_DATABASE}",
        "arn:aws:glue:${AWS_REGION}:${AWS_ACCOUNT_ID}:table/${GLUE_DATABASE}/*"
      ]
    }
  ]
}
EOF

aws iam put-role-policy \
  --role-name ${GLUE_ROLE} \
  --policy-name "${GLUE_ROLE}-custom-policy" \
  --policy-document file://glue-custom-policy.json
```

## Step 6: Glue Streaming ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ

ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã® Spark Structured Streaming ã‚³ãƒ¼ãƒ‰ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€AWS ç’°å¢ƒç”¨ã«ä¿®æ­£ã—ã¾ã™ã€‚

```bash
cat > glue_streaming_iceberg.py << 'EOF'
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import from_json, col, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, TimestampType

# ã‚¸ãƒ§ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'kinesis_stream_name',
    'database_name',
    'table_name',
    's3_warehouse_path'
])

# SparkContext / GlueContext åˆæœŸåŒ–
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
kinesis_stream_name = args['kinesis_stream_name']
database_name = args['database_name']
table_name = args['table_name']
s3_warehouse_path = args['s3_warehouse_path']
region = "ap-northeast-1"

# Iceberg ã‚«ã‚¿ãƒ­ã‚°è¨­å®šï¼ˆGlue Data Catalog ä½¿ç”¨ï¼‰
# Athena ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚«ã‚¿ãƒ­ã‚°åã¨ä¸€è‡´ã•ã›ã‚‹ã“ã¨ã§ã€ã‚¯ã‚¨ãƒªæ™‚ã®æŒ‡å®šãŒç°¡æ½”ã«ãªã‚‹
catalog_name = "awsdatacatalog"
spark.conf.set(f"spark.sql.catalog.{catalog_name}", "org.apache.iceberg.spark.SparkCatalog")
spark.conf.set(f"spark.sql.catalog.{catalog_name}.warehouse", s3_warehouse_path)
spark.conf.set(f"spark.sql.catalog.{catalog_name}.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
spark.conf.set(f"spark.sql.catalog.{catalog_name}.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")

# ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
full_table_name = f"{catalog_name}.{database_name}.{table_name}"
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS {full_table_name} (
        event_id STRING,
        user_id STRING,
        event_type STRING,
        page STRING,
        event_time TIMESTAMP,
        processed_time TIMESTAMP
    )
    USING iceberg
""")

print(f"Table {full_table_name} is ready")

# JSON ã‚¹ã‚­ãƒ¼ãƒå®šç¾©ï¼ˆå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ï¼‰
schema = StructType([
    StructField("event_id", StringType()),
    StructField("user_id", StringType()),
    StructField("event_type", StringType()),
    StructField("page", StringType()),
    StructField("timestamp", StringType())
])

# Kinesis ã‹ã‚‰ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èª­ã¿è¾¼ã¿
kinesis_df = spark.readStream \
    .format("kinesis") \
    .option("streamName", kinesis_stream_name) \
    .option("endpointUrl", f"https://kinesis.{region}.amazonaws.com") \
    .option("startingPosition", "TRIM_HORIZON") \
    .load()

# ãƒ‡ãƒ¼ã‚¿å¤‰æ›
# - Kinesis ã® data ã‚«ãƒ©ãƒ ã¯ Base64 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ãŸã‚æ–‡å­—åˆ—ã«ã‚­ãƒ£ã‚¹ãƒˆ
# - JSON ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ§‹é€ åŒ–
# - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å‹ã«å¤‰æ›
parsed_df = kinesis_df \
    .selectExpr("CAST(data AS STRING) as json_data") \
    .select(from_json(col("json_data"), schema).alias("data")) \
    .select(
        col("data.event_id"),
        col("data.user_id"),
        col("data.event_type"),
        col("data.page"),
        col("data.timestamp").cast(TimestampType()).alias("event_time"),
        current_timestamp().alias("processed_time")
    )

# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç”¨ãƒ‘ã‚¹
checkpoint_path = f"{s3_warehouse_path}/checkpoints/{table_name}"

# Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ›¸ãè¾¼ã¿
query = parsed_df.writeStream \
    .format("iceberg") \
    .outputMode("append") \
    .option("checkpointLocation", checkpoint_path) \
    .toTable(full_table_name)

query.awaitTermination()

job.commit()
EOF
```

### ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã¨ã®é•ã„

| é …ç›® | ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ | AWS ç’°å¢ƒ |
|------|-------------|----------|
| ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚½ãƒ¼ã‚¹ | `.format("kafka")` | `.format("kinesis")` |
| ã‚«ã‚¿ãƒ­ã‚°å®Ÿè£… | `RESTCatalog` | `GlueCatalog` |
| ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ | MinIOï¼ˆS3 äº’æ›ï¼‰ | Amazon S3 |
| ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ | ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ  | S3 |

åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆJSON ãƒ‘ãƒ¼ã‚¹ã€å‹å¤‰æ›ï¼‰ã¯ã»ã¼åŒã˜ã§ã™ã€‚

## Step 7: ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨ Job ä½œæˆ

ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ S3 ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰:

```bash
aws s3 cp glue_streaming_iceberg.py s3://${S3_BUCKET_SCRIPTS}/scripts/
```

Glue Streaming Job ã‚’ä½œæˆ:

```bash
aws glue create-job \
  --name "${PREFIX}-job" \
  --role "arn:aws:iam::${AWS_ACCOUNT_ID}:role/${GLUE_ROLE}" \
  --command '{
    "Name": "gluestreaming",
    "ScriptLocation": "s3://'"${S3_BUCKET_SCRIPTS}"'/scripts/glue_streaming_iceberg.py",
    "PythonVersion": "3"
  }' \
  --default-arguments '{
    "--job-language": "python",
    "--datalake-formats": "iceberg",
    "--kinesis_stream_name": "'"${KINESIS_STREAM}"'",
    "--database_name": "'"${GLUE_DATABASE}"'",
    "--table_name": "streaming_events",
    "--s3_warehouse_path": "s3://'"${S3_BUCKET_DATA}"'/warehouse"
  }' \
  --glue-version "5.0" \
  --number-of-workers 2 \
  --worker-type "G.1X"
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- `--glue-version "5.0"`: 2024å¹´12æœˆã«ä¸€èˆ¬æä¾›é–‹å§‹ã•ã‚ŒãŸæœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³
- `--datalake-formats iceberg`: Iceberg ã‚µãƒãƒ¼ãƒˆã‚’æœ‰åŠ¹åŒ–
- `gluestreaming`: Streaming Job ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®š
- `G.1X x 2 workers`: æœ€å°æ§‹æˆï¼ˆæ¤œè¨¼ç”¨ï¼‰

## Step 8: Job ã®å®Ÿè¡Œ

```bash
aws glue start-job-run --job-name "${PREFIX}-job"
```

ã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹ã‚’ç¢ºèªï¼ˆRUNNING ã«ãªã‚‹ã¾ã§ 2ã€œ3 åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰:

```bash
while true; do
  STATE=$(aws glue get-job-runs --job-name "${PREFIX}-job" \
    --query 'JobRuns[0].JobRunState' --output text)
  echo "Job state: ${STATE}"
  if [ "$STATE" = "RUNNING" ]; then
    break
  fi
  sleep 10
done
```

## Step 9: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®é€ä¿¡

ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ Kinesis ã«é€ä¿¡ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

```bash
cat > send_test_events.sh << 'SCRIPT'
#!/bin/bash
EVENT_TYPES=("click" "page_view" "purchase")

for i in {1..10}; do
  EVENT_TYPE=${EVENT_TYPES[$((RANDOM % 3))]}
  DATA=$(cat << EOF
{
  "event_id": "evt_$(date +%s)_${i}",
  "user_id": "user_00$((RANDOM % 5))",
  "event_type": "${EVENT_TYPE}",
  "page": "/products/$((100 + i))",
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
}
EOF
)
  
  aws kinesis put-record \
    --stream-name ${KINESIS_STREAM} \
    --partition-key "pk_${i}" \
    --data "$(echo -n ${DATA} | base64)"
  
  echo "Sent event ${i}: ${EVENT_TYPE}"
  sleep 2
done
SCRIPT
```

å®Ÿè¡Œ:

```bash
chmod +x send_test_events.sh
./send_test_events.sh
```

## Step 10: Athena ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª

2ã€œ3 åˆ†å¾…ã£ã¦ã‹ã‚‰ã€Athena ã§ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
QUERY_ID=$(aws athena start-query-execution \
  --query-string "SELECT * FROM ${GLUE_DATABASE}.streaming_events ORDER BY event_time DESC LIMIT 10" \
  --result-configuration "OutputLocation=s3://${S3_BUCKET_DATA}/athena-results/" \
  --work-group "primary" \
  --query 'QueryExecutionId' --output text)

echo "Query ID: ${QUERY_ID}"
echo "Waiting for query to complete..."
sleep 5

aws athena get-query-results --query-execution-id ${QUERY_ID} \
  --query 'ResultSet.Rows[*].Data[*].VarCharValue'
```

å‡ºåŠ›ä¾‹:

```json
[
    ["event_id", "user_id", "event_type", "page", "event_time", "processed_time"],
    ["evt_1767884412_10", "user_000", "click", "/products/110", "2026-01-08T15:16:52Z", "2026-01-08T15:17:01Z"],
    ["evt_1767884410_9", "user_004", "page_view", "/products/109", "2026-01-08T15:16:50Z", "2026-01-08T15:17:01Z"],
    ...
]
```

## Step 11: Job ã®åœæ­¢ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

### Streaming Job ã®åœæ­¢

```bash
JOB_RUN_ID=$(aws glue get-job-runs --job-name "${PREFIX}-job" \
  --query 'JobRuns[?JobRunState==`RUNNING`].Id' --output text)

if [ -n "$JOB_RUN_ID" ]; then
  aws glue batch-stop-job-run \
    --job-name "${PREFIX}-job" \
    --job-run-ids ${JOB_RUN_ID}
fi
```

### ãƒªã‚½ãƒ¼ã‚¹ã®å‰Šé™¤

Glue Job:

```bash
aws glue delete-job --job-name "${PREFIX}-job"
```

Kinesis Stream:

```bash
aws kinesis delete-stream --stream-name ${KINESIS_STREAM}
```

Glue Table ã¨ Database:

```bash
aws glue delete-table --database-name ${GLUE_DATABASE} --name "streaming_events"
aws glue delete-database --name ${GLUE_DATABASE}
```

S3 ãƒã‚±ãƒƒãƒˆ:

```bash
aws s3 rm s3://${S3_BUCKET_SCRIPTS}/ --recursive
aws s3 rb s3://${S3_BUCKET_SCRIPTS}
aws s3 rm s3://${S3_BUCKET_DATA}/ --recursive
aws s3 rb s3://${S3_BUCKET_DATA}
```

IAM ãƒ­ãƒ¼ãƒ«:

```bash
aws iam detach-role-policy --role-name ${GLUE_ROLE} \
  --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
aws iam detach-role-policy --role-name ${GLUE_ROLE} \
  --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
aws iam detach-role-policy --role-name ${GLUE_ROLE} \
  --policy-arn arn:aws:iam::aws:policy/AmazonKinesisReadOnlyAccess
aws iam delete-role-policy --role-name ${GLUE_ROLE} \
  --policy-name "${GLUE_ROLE}-custom-policy"
aws iam delete-role --role-name ${GLUE_ROLE}
```

## ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã¨ã®æ¯”è¼ƒ

### å…±é€šç‚¹

| é …ç›® | èª¬æ˜ |
|------|------|
| å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ | Apache Sparkï¼ˆStructured Streamingï¼‰ |
| ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ | Apache Iceberg |
| æ›¸ãè¾¼ã¿æ–¹å¼ | `.writeStream.format("iceberg").toTable()` |
| ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ | éšœå®³å¾©æ—§ã®ãŸã‚ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆç®¡ç† |

### ç›¸é•ç‚¹

| é …ç›® | ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ | AWS ç’°å¢ƒ |
|------|-------------|----------|
| èµ·å‹•æ™‚é–“ | æ•°ç§’ | 2ã€œ3 åˆ†ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿èµ·å‹•ï¼‰ |
| ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° | æ‰‹å‹• | è‡ªå‹•ï¼ˆDPU æ•°ã§èª¿æ•´ï¼‰ |
| ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç† | REST Catalogï¼ˆæ‰‹å‹•ç®¡ç†ï¼‰ | Glue Data Catalogï¼ˆãƒãƒãƒ¼ã‚¸ãƒ‰ï¼‰ |
| é‹ç”¨ã‚³ã‚¹ãƒˆ | ç„¡æ–™ | DPU æ™‚é–“èª²é‡‘ |
| ã‚¯ã‚¨ãƒªã‚¨ãƒ³ã‚¸ãƒ³ | Spark SQL / Trino | Athena / Redshift Spectrum |

### ã‚³ã‚¹ãƒˆã«é–¢ã™ã‚‹æ³¨æ„

Glue Streaming Job ã¯ **å®Ÿè¡Œæ™‚é–“ã«å¿œã˜ã¦èª²é‡‘** ã•ã‚Œã¾ã™ã€‚æ¤œè¨¼å¾Œã¯å¿…ãš Job ã‚’åœæ­¢ã—ã¦ãã ã•ã„ã€‚

- G.1X ãƒ¯ãƒ¼ã‚«ãƒ¼: ç´„ $0.44/DPU-hour
- 2 ãƒ¯ãƒ¼ã‚«ãƒ¼ Ã— 1 æ™‚é–“ = ç´„ $0.88

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Job ãŒ FAILED ã«ãªã‚‹

1. CloudWatch Logs ã§ã‚¨ãƒ©ãƒ¼ã‚’ç¢ºèª:

```bash
aws logs describe-log-streams \
  --log-group-name "/aws-glue/jobs/output" \
  --order-by LastEventTime --descending --limit 1
```

2. ã‚ˆãã‚ã‚‹åŸå› :
   - IAM æ¨©é™ä¸è¶³ï¼ˆS3ã€Kinesisã€Glue Catalogï¼‰
   - Kinesis Stream ãŒå­˜åœ¨ã—ãªã„
   - S3 ãƒ‘ã‚¹ã®æŒ‡å®šãƒŸã‚¹

### ãƒ‡ãƒ¼ã‚¿ãŒæ›¸ãè¾¼ã¾ã‚Œãªã„

1. Kinesis ã«ãƒ‡ãƒ¼ã‚¿ãŒåˆ°é”ã—ã¦ã„ã‚‹ã‹ç¢ºèª:

```bash
aws kinesis get-shard-iterator \
  --stream-name ${KINESIS_STREAM} \
  --shard-id shardId-000000000000 \
  --shard-iterator-type TRIM_HORIZON

aws kinesis get-records --shard-iterator <iterator>
```

2. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚¯ãƒªã‚¢ã—ã¦å†å®Ÿè¡Œ:

```bash
aws s3 rm s3://${S3_BUCKET_DATA}/warehouse/checkpoints/ --recursive
```

:::message alert
**æ³¨æ„**: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å‰Šé™¤ã™ã‚‹ã¨ã€æ¬¡å› Job å®Ÿè¡Œæ™‚ã« Kinesis ã®æœ€åˆï¼ˆTRIM_HORIZONï¼‰ã‹ã‚‰å†èª­ã¿è¾¼ã¿ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ—¢ã«å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãŒå†åº¦æ›¸ãè¾¼ã¾ã‚Œã€**ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ãŒç™ºç”Ÿ**ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯æ…é‡ã«åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
:::

### Lake Formation æ¨©é™ã‚¨ãƒ©ãƒ¼

Athena ã‚¯ã‚¨ãƒªã§ `Insufficient Lake Formation permission(s)` ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆ:

ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ Lake Formation ç®¡ç†è€…ã«è¿½åŠ :

```bash
CURRENT_USER_ARN=$(aws sts get-caller-identity --query 'Arn' --output text)

aws lakeformation put-data-lake-settings \
  --data-lake-settings '{
    "DataLakeAdmins": [
      {"DataLakePrincipalIdentifier": "'"${CURRENT_USER_ARN}"'"}
    ],
    "CreateDatabaseDefaultPermissions": [
      {
        "Principal": {"DataLakePrincipalIdentifier": "IAM_ALLOWED_PRINCIPALS"},
        "Permissions": ["ALL"]
      }
    ],
    "CreateTableDefaultPermissions": [
      {
        "Principal": {"DataLakePrincipalIdentifier": "IAM_ALLOWED_PRINCIPALS"},
        "Permissions": ["ALL"]
      }
    ]
  }'
```

ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®æ¨©é™ã‚’ä»˜ä¸:

```bash
aws lakeformation grant-permissions \
  --principal DataLakePrincipalIdentifier="IAM_ALLOWED_PRINCIPALS" \
  --resource '{"Table": {"DatabaseName": "'"${GLUE_DATABASE}"'", "Name": "streaming_events"}}' \
  --permissions "ALL"
```

## ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§æ¤œè¨¼ã—ãŸ Spark Structured Streaming + Kafka + Iceberg ã®æ§‹æˆã‚’ã€AWS ç’°å¢ƒï¼ˆKinesis + Glue Streaming + Icebergï¼‰ã§å®Ÿè£…ã—ã¾ã—ãŸã€‚

**æ¤œè¨¼ã§ããŸã“ã¨**:
- Kinesis Data Streams ã‹ã‚‰ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èª­ã¿è¾¼ã¿
- Glue Streaming Job ã§ã® Spark Structured Streaming å®Ÿè¡Œ
- Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›¸ãè¾¼ã¿
- Glue Data Catalog ã«ã‚ˆã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†
- Athena ã§ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œ

**ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã¨ã®é•ã„**:
- ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚½ãƒ¼ã‚¹ãŒ Kafka ã‹ã‚‰ Kinesis ã«å¤‰æ›´
- ã‚«ã‚¿ãƒ­ã‚°ãŒ REST Catalog ã‹ã‚‰ Glue Data Catalog ã«å¤‰æ›´
- ãƒãƒãƒ¼ã‚¸ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã«ã‚ˆã‚‹é‹ç”¨è² è·ã®è»½æ¸›

ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§åŸ¹ã£ãŸ Spark Structured Streaming ã®çŸ¥è­˜ã¯ã€ãã®ã¾ã¾ AWS ç’°å¢ƒã§ã‚‚æ´»ç”¨ã§ãã¾ã™ã€‚

## è£œè¶³: Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ›¸ãè¾¼ã¿ã‚’ç¶šã‘ã‚‹ã¨ã€å°ã•ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§é‡ã«ä½œæˆã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Š Athena ã®ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

æœ¬ç•ªç’°å¢ƒã§ã¯ã€ä»¥ä¸‹ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å‡¦ç†ã‚’å®šæœŸçš„ã«å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™:

- **OPTIMIZEï¼ˆã‚³ãƒ³ãƒ‘ã‚¯ã‚·ãƒ§ãƒ³ï¼‰**: å°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã—ã¦å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã«ã¾ã¨ã‚ã‚‹
- **VACUUM**: ä¸è¦ã«ãªã£ãŸå¤ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚„ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹

```sql
-- ã‚³ãƒ³ãƒ‘ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆAthena ã§å®Ÿè¡Œï¼‰
OPTIMIZE awsdatacatalog.glue_streaming_iceberg_db.streaming_events REWRITE DATA USING BIN_PACK;

-- å¤ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®å‰Šé™¤ï¼ˆ7æ—¥ä»¥ä¸Šå‰ã®ã‚‚ã®ã‚’å‰Šé™¤ï¼‰
VACUUM awsdatacatalog.glue_streaming_iceberg_db.streaming_events;
```

ã“ã‚Œã‚‰ã®å‡¦ç†ã¯ã€EventBridge + Lambda ã‚„ Step Functions ã§å®šæœŸå®Ÿè¡Œã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚

## è£œè¶³: S3 Tables ã«ã¤ã„ã¦

AWS ã§ã¯ 2024 å¹´ 12 æœˆã« **Amazon S3 Tables** ã¨ã„ã†æ–°ã—ã„æ©Ÿèƒ½ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã¾ã—ãŸã€‚S3 Tables ã¯ Iceberg ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•ç®¡ç†ã—ã€ã‚³ãƒ³ãƒ‘ã‚¯ã‚·ãƒ§ãƒ³ã‚‚è‡ªå‹•åŒ–ã•ã‚Œã¾ã™ã€‚

ä»Šåº¦æ¤œè¨¼ã—ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚

## å‚è€ƒãƒªãƒ³ã‚¯

- [AWS Glue Streaming ETL](https://docs.aws.amazon.com/glue/latest/dg/add-job-streaming.html)
- [Using the Iceberg framework in AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-iceberg.html)
- [GitHub - aws-samples/aws-glue-streaming-etl-with-apache-iceberg](https://github.com/aws-samples/aws-glue-streaming-etl-with-apache-iceberg)
