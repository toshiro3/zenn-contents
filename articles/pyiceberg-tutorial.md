---
title: "Dockerä¸è¦ï¼uv + PyIcebergã§Apache Icebergã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç†è§£ã™ã‚‹"
emoji: "ğŸ§Š"
type: "tech"
topics: ["iceberg", "python", "uv", "datalake", "pyiceberg"]
published: true
---

## ã¯ã˜ã‚ã«

Apache Icebergã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚¤ã‚¯ä¸Šã§ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹ã®ã‚ˆã†ãªæ©Ÿèƒ½ã‚’å®Ÿç¾ã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã™ã€‚AWS S3 Tablesã€Snowflakeã€BigQueryãªã©ä¸»è¦ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã§ã‚‚æ¡ç”¨ãŒé€²ã‚“ã§ã„ã‚‹ã‚ˆã†ã§ã™ã€‚

ã“ã®è¨˜äº‹ã§ã¯ã€ã€Œã¾ãšã¯Icebergã®ä»•çµ„ã¿ã‚’è»½ãç†è§£ã—ãŸã„ã€ã¨ã„ã†ã“ã¨ã‚’å„ªå…ˆã—ã¦ã€**Dockerä¸è¦**ã§ã€**uv + PyIceberg**ã ã‘ã‚’ä½¿ã£ã¦Icebergã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç†è§£ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

### å¯¾è±¡èª­è€…

- Apache Icebergã®ä»•çµ„ã¿ã‚’ç†è§£ã—ãŸã„æ–¹
- Dockerç’°å¢ƒãªã—ã§è»½é‡ã«Icebergã‚’è©¦ã—ãŸã„æ–¹
- S3 Tablesã‚„Athenaã‚’ä½¿ã†å‰ã«Icebergã®åŸºç¤ã‚’æŠ¼ã•ãˆãŸã„æ–¹
- uvã‚’è©¦ã—ã¦ã¿ãŸã„æ–¹

### ã“ã®è¨˜äº‹ã§å­¦ã¹ã‚‹ã“ã¨

- Icebergã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆmetadata.jsonã€ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒªã‚¹ãƒˆã€ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
- ãƒ‡ãƒ¼ã‚¿è¿½åŠ ãƒ»æ›´æ–°ãƒ»å‰Šé™¤æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«å¤‰åŒ–
- ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã¨ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«ã®ä»•çµ„ã¿

## ç’°å¢ƒæ§‹ç¯‰

### uvã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

uvã¯Rustè£½ã®é«˜é€ŸãªPythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ã™ã€‚pip + venv + pyenvã®æ©Ÿèƒ½ã‚’çµ±åˆã—ã¦ãŠã‚Šã€ä¾å­˜è§£æ±ºã‚‚é«˜é€Ÿã§ã™ã€‚

```bash
# macOS (Homebrew)
brew install uv

# ã¾ãŸã¯å…¬å¼ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼
curl -LsSf https://astral.sh/uv/install.sh | sh
```

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèªï¼š

```bash
$ which uv
/opt/homebrew/bin/uv  # Homebrewã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå ´åˆ
```

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ

```bash
mkdir iceberg-tutorial
cd iceberg-tutorial

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
uv init

# Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³æŒ‡å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦uvãŒè‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰
uv python pin 3.12

# PyIcebergã¨ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’è¿½åŠ 
uv add "pyiceberg[pyarrow,sql-sqlite]"
uv add pandas
```

ç’°å¢ƒãŒæ­£ã—ãæ§‹ç¯‰ã•ã‚ŒãŸã‹ç¢ºèªã—ã¾ã™ï¼š

```bash
$ uv run python --version
Using CPython 3.12.8 interpreter at: /opt/homebrew/opt/python@3.12/bin/python3.12
Creating virtual environment at: .venv
Python 3.12.8
```

ã“ã‚Œã§`pyproject.toml`ã¨`uv.lock`ãŒç”Ÿæˆã•ã‚Œã€ä¾å­˜é–¢ä¿‚ãŒè§£æ±ºã•ã‚Œã¾ã™ã€‚

## Icebergã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆæ¦‚è¦ï¼‰

æœ¬é¡Œã«å…¥ã‚‹å‰ã«ã€Icebergã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç°¡å˜ã«èª¬æ˜ã—ã¾ã™ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ã‚«ã‚¿ãƒ­ã‚°å±¤                          â”‚  â† ã€Œç¾åœ¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ã©ã“ã‹ã€ã‚’ç®¡ç†
â”‚  (Glue Data Catalog, SQLiteç­‰)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å±¤                        â”‚  â† ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã€ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±
â”‚  (metadata.json, manifest list/file)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ãƒ‡ãƒ¼ã‚¿å±¤                            â”‚  â† å®Ÿéš›ã®Parquetãƒ•ã‚¡ã‚¤ãƒ«
â”‚  (Parquetãƒ•ã‚¡ã‚¤ãƒ«ç¾¤)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

AWSã§ã„ãˆã°ã€ã‚«ã‚¿ãƒ­ã‚°å±¤ãŒGlue Data Catalogã€ãƒ‡ãƒ¼ã‚¿å±¤ãŒS3ã«å¯¾å¿œã—ã¾ã™ã€‚ä»Šå›ã¯ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§SQLiteã‚’ã‚«ã‚¿ãƒ­ã‚°ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚

## ãƒãƒ³ã‚ºã‚ªãƒ³

ã“ã“ã‹ã‚‰ã¯Python REPLã§ä¸€ã¤ãšã¤ç¢ºèªã—ãªãŒã‚‰é€²ã‚ã¾ã™ã€‚

```bash
uv run python
```

### Step 1: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚«ã‚¿ãƒ­ã‚°è¨­å®š

```python
from pyiceberg.catalog import load_catalog
from pyiceberg.schema import Schema
from pyiceberg.types import StringType, LongType, NestedField
from pathlib import Path

# warehouseãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æº–å‚™
warehouse_path = Path("./warehouse").absolute()
warehouse_path.mkdir(exist_ok=True)
print(f"Warehouse: {warehouse_path}")
```

```
Warehouse: /Users/yourname/iceberg-tutorial/warehouse
```

### Step 2: ã‚«ã‚¿ãƒ­ã‚°ã®ä½œæˆ

```python
catalog = load_catalog(
    "local",
    **{
        "type": "sql",
        "uri": "sqlite:///iceberg_catalog.db",
        "warehouse": f"file://{warehouse_path}"
    }
)

print(catalog)
```

```
local (<class 'pyiceberg.catalog.sql.SqlCatalog'>)
```

ã“ã®æ™‚ç‚¹ã§ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã«`iceberg_catalog.db`ï¼ˆSQLiteãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ãŒä½œæˆã•ã‚Œã¾ã™ã€‚ã“ã‚ŒãŒã‚«ã‚¿ãƒ­ã‚°ã¨ã—ã¦ã€Œã©ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒã©ã“ã«ã‚ã‚‹ã‹ã€ã‚’ç®¡ç†ã—ã¾ã™ã€‚

### Step 3: ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ã®ä½œæˆ

```python
# æ—¢å­˜ã®ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ç¢ºèª
print(catalog.list_namespaces())

# ãƒãƒ¼ãƒ ã‚¹ãƒšãƒ¼ã‚¹ä½œæˆ
catalog.create_namespace("tutorial")

# ä½œæˆå¾Œã®ç¢ºèª
print(catalog.list_namespaces())
```

```
[]
[('tutorial',)]
```

### Step 4: ã‚¹ã‚­ãƒ¼ãƒå®šç¾©

```python
schema = Schema(
    NestedField(1, "user_id", LongType(), required=True),
    NestedField(2, "name", StringType(), required=True),
    NestedField(3, "email", StringType(), required=False),
)

# ã‚¹ã‚­ãƒ¼ãƒã®ä¸­èº«ã‚’ç¢ºèª
for field in schema.fields:
    print(f"  {field.field_id}: {field.name} ({field.field_type}) required={field.required}")
```

```
  1: user_id (long) required=True
  2: name (string) required=True
  3: email (string) required=False
```

`required=True`ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯NULLã‚’è¨±å®¹ã—ã¾ã›ã‚“ã€‚ã“ã‚Œã¯å¾Œã®ãƒ‡ãƒ¼ã‚¿è¿½åŠ æ™‚ã«é‡è¦ã«ãªã‚Šã¾ã™ã€‚

### Step 5: ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ

```python
table = catalog.create_table("tutorial.users", schema=schema)
print(table)
```

```
users(
  1: user_id: required long,
  2: name: required string,
  3: email: optional string
)
```

### Step 6: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª

```python
metadata = table.metadata

print(f"ãƒ†ãƒ¼ãƒ–ãƒ«UUID: {metadata.table_uuid}")
print(f"ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³: {metadata.format_version}")
print(f"ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³: {metadata.location}")
```

```
ãƒ†ãƒ¼ãƒ–ãƒ«UUID: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³: 2
ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³: file:///Users/yourname/iceberg-tutorial/warehouse/tutorial/users
```

### Step 7: ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã®ç¢ºèª

ã“ã®æ™‚ç‚¹ã§åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰`warehouse`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```bash
$ tree warehouse
warehouse
â””â”€â”€ tutorial
    â””â”€â”€ users
        â””â”€â”€ metadata
            â””â”€â”€ 00000-xxxxxxxx.metadata.json
```

ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆç›´å¾Œã¯ã€`metadata`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æœ€åˆã®`metadata.json`ã ã‘ãŒå­˜åœ¨ã—ã¾ã™ã€‚ã¾ã ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦ã„ãªã„ã®ã§`data`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

### Step 8: metadata.jsonã®ä¸­èº«ã‚’ç¢ºèª

```python
import json

metadata_dir = warehouse_path / "tutorial" / "users" / "metadata"
metadata_files = sorted(metadata_dir.glob("*.metadata.json"))
print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {[f.name for f in metadata_files]}")

# metadata.jsonã‚’èª­ã‚€
with open(metadata_files[-1]) as f:
    meta = json.load(f)

print(json.dumps(meta, indent=2)[:500])  # é•·ã„ã®ã§å…ˆé ­éƒ¨åˆ†ã ã‘
```

`metadata.json`ã«ã¯ã‚¹ã‚­ãƒ¼ãƒå®šç¾©ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä»•æ§˜ã€ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå±¥æ­´ãªã©ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 

### Step 9: PyArrowã§ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¦è¿½åŠ 

PyIcebergã§ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã™ã‚‹éš›ã¯ã€PyArrowã®Tableã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python
import pyarrow as pa

# ã‚¹ã‚­ãƒ¼ãƒã‚’æ˜ç¤ºçš„ã«å®šç¾©ï¼ˆrequired/optionalã‚’åˆã‚ã›ã‚‹ï¼‰
arrow_schema = pa.schema([
    pa.field("user_id", pa.int64(), nullable=False),
    pa.field("name", pa.string(), nullable=False),
    pa.field("email", pa.string(), nullable=True),
])

data = pa.table({
    "user_id": [1, 2, 3],
    "name": ["Alice", "Bob", "Charlie"],
    "email": ["alice@example.com", "bob@example.com", None],
}, schema=arrow_schema)

table.append(data)
print("3ä»¶è¿½åŠ ã—ã¾ã—ãŸ")
```

:::message alert
PyArrowã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã™ã‚‹éš›ã€ã‚¹ã‚­ãƒ¼ãƒã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ãªã„ã¨å…¨ã‚«ãƒ©ãƒ ãŒ`nullable=True`ï¼ˆoptionalï¼‰ã«ãªã‚Šã¾ã™ã€‚Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒã§`required=True`ã¨ã—ãŸã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã€ã‚¹ã‚­ãƒ¼ãƒä¸ä¸€è‡´ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚

```
ValueError: Mismatch in fields:
â”ƒ    â”ƒ Table field               â”ƒ Dataframe field           â”ƒ
â”‚ âŒ â”‚ 1: user_id: required long â”‚ 1: user_id: optional long â”‚
```

ã“ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ã€`pa.schema()`ã§ã‚¹ã‚­ãƒ¼ãƒã‚’æ˜ç¤ºçš„ã«å®šç¾©ã—ã¦ãã ã•ã„ã€‚
:::

### Step 10: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®ç¢ºèª

```python
table.refresh()  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å†èª­ã¿è¾¼ã¿

for snapshot in table.metadata.snapshots:
    print(f"Snapshot ID: {snapshot.snapshot_id}")
    print(f"  Operation: {snapshot.summary.get('operation')}")
    print(f"  Added records: {snapshot.summary.get('added-records')}")
```

```
Snapshot ID: 5815774430499421792
  Operation: Operation.APPEND
  Added records: 3
```

### Step 11: è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã§ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å¢—ã‚„ã™

```python
more_data = pa.table({
    "user_id": [4, 5],
    "name": ["David", "Eve"],
    "email": ["david@example.com", "eve@example.com"],
}, schema=arrow_schema)

table.append(more_data)
table.refresh()

print(f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ•°: {len(table.metadata.snapshots)}")
```

```
ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ•°: 2
```

### Step 12: ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã®å¤‰åŒ–ã‚’ç¢ºèª

åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§`warehouse`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèªã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã«ã‚ˆã£ã¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¢—ãˆã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

```bash
$ tree warehouse
warehouse
â””â”€â”€ tutorial
    â””â”€â”€ users
        â”œâ”€â”€ data
        â”‚   â”œâ”€â”€ 00000-0-xxxxxxxx.parquet
        â”‚   â””â”€â”€ 00000-0-yyyyyyyy.parquet
        â””â”€â”€ metadata
            â”œâ”€â”€ 00000-xxxxxxxx.metadata.json
            â”œâ”€â”€ 00001-xxxxxxxx.metadata.json
            â”œâ”€â”€ 00002-xxxxxxxx.metadata.json
            â”œâ”€â”€ xxxxxxxx-m0.avro
            â”œâ”€â”€ yyyyyyyy-m0.avro
            â”œâ”€â”€ snap-xxxx-0-xxxxxxxx.avro
            â””â”€â”€ snap-yyyy-0-yyyyyyyy.avro
```

## ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¹å‰²

Step 12ã§ç¢ºèªã—ãŸãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«ã¤ã„ã¦ã€ãã‚Œãã‚Œã®å½¹å‰²ã‚’èª¬æ˜ã—ã¾ã™ã€‚

| ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥ | èª¬æ˜ |
|-------------|------|
| `*.parquet` | å®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆã‚«ãƒ©ãƒ ãƒŠãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰ |
| `*.metadata.json` | ãƒ†ãƒ¼ãƒ–ãƒ«çŠ¶æ…‹ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã€‚æ“ä½œã”ã¨ã«æ–°è¦ä½œæˆ |
| `snap-*.avro` | ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒªã‚¹ãƒˆã€‚ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ |
| `*-m0.avro` | ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã€‚Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ã¨çµ±è¨ˆæƒ…å ± |

éšå±¤æ§‹é€ ï¼š

```
metadata.jsonï¼ˆç¾åœ¨ã®çŠ¶æ…‹ï¼‰
    â†“ å‚ç…§
snap-*.avroï¼ˆãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒªã‚¹ãƒˆï¼‰
    â†“ å‚ç…§
*-m0.avroï¼ˆãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    â†“ å‚ç…§
data/*.parquetï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
```

## ãƒ¬ã‚³ãƒ¼ãƒ‰ã®æ›´æ–°

### Step 13: ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª

```python
df = table.scan().to_pandas()
print(df)
```

```
   user_id     name              email
0        1    Alice  alice@example.com
1        2      Bob    bob@example.com
2        3  Charlie               None
3        4    David  david@example.com
4        5      Eve    eve@example.com
```

### Step 14: ãƒ¬ã‚³ãƒ¼ãƒ‰ã®æ›´æ–°

PyIcebergã§ã¯`overwrite`ã‚’ä½¿ã£ã¦æ¡ä»¶ã«åˆã†ãƒ‡ãƒ¼ã‚¿ã‚’ç½®ãæ›ãˆã¾ã™ã€‚

```python
# user_id=2 (Bob) ã®emailã‚’æ›´æ–°
updated_data = pa.table({
    "user_id": [2],
    "name": ["Bob"],
    "email": ["bob.new@example.com"],
}, schema=arrow_schema)

table.overwrite(updated_data, overwrite_filter="user_id == 2")
print("æ›´æ–°å®Œäº†")
```

### Step 15: æ›´æ–°å¾Œã®ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª

```python
table.refresh()
df = table.scan().to_pandas()
print(df)
```

```
   user_id     name                email
0        2      Bob  bob.new@example.com
1        1    Alice    alice@example.com
2        3  Charlie                 None
3        4    David    david@example.com
4        5      Eve      eve@example.com
```

Bobã®emailãŒæ›´æ–°ã•ã‚Œã¦ã„ã¾ã™ã€‚

```python
for snap in table.metadata.snapshots:
    print(f"Snapshot ID: {snap.snapshot_id}")
    print(f"  Operation: {snap.summary.get('operation')}")
    print(f"  Added files: {snap.summary.get('added-data-files', 0)}")
    print(f"  Deleted files: {snap.summary.get('deleted-data-files', 0)}")
    print()
```

```
Snapshot ID: 5815774430499421792
  Operation: Operation.APPEND
  Added files: 1
  Deleted files: None

Snapshot ID: 3879896634347338753
  Operation: Operation.APPEND
  Added files: 1
  Deleted files: None

Snapshot ID: 5929255525955454062
  Operation: Operation.OVERWRITE
  Added files: 1
  Deleted files: 1

Snapshot ID: 7234342154326247462
  Operation: Operation.APPEND
  Added files: 1
  Deleted files: None
```

`overwrite`æ“ä½œã§ã¯ã€å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ãŒè«–ç†å‰Šé™¤ï¼ˆ`Deleted files: 1`ï¼‰ã•ã‚Œã€æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¿½åŠ ï¼ˆ`Added files: 1`ï¼‰ã•ã‚Œã¾ã™ã€‚ã“ã‚ŒãŒ**Copy-on-Write**æ–¹å¼ã§ã™ã€‚

:::message
Bobã ã‘ã‚’æ›´æ–°ã—ãŸã¤ã‚‚ã‚Šã§ã‚‚ã€åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã¾ã‚Œã¦ã„ãŸAliceã€Bobã€Charlieã®3ãƒ¬ã‚³ãƒ¼ãƒ‰ã™ã¹ã¦ãŒè«–ç†å‰Šé™¤ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆã•ã‚Œã¾ã™ã€‚ãŸã ã—ã€Aliceã¨Charlieã¯æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã«å†æ›¸ãè¾¼ã¿ã•ã‚Œã‚‹ãŸã‚ã€å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ãŒå¤±ã‚ã‚Œã‚‹ã‚ã‘ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å‰Šé™¤æ™‚ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã§`deleted-records`ã‚’ç¢ºèªã™ã‚‹ã¨ã€ã“ã®æ•°å€¤ã‚’ç¢ºèªã§ãã¾ã™ã€‚
:::

## ãƒ¬ã‚³ãƒ¼ãƒ‰ã®å‰Šé™¤

### Step 16: ãƒ¬ã‚³ãƒ¼ãƒ‰ã®å‰Šé™¤

PyIcebergã§ã¯`delete`ãƒ¡ã‚½ãƒƒãƒ‰ã§æ¡ä»¶ã«åˆã†ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã§ãã¾ã™ã€‚

```python
# user_id=3 (Charlie) ã‚’å‰Šé™¤
table.delete(delete_filter="user_id == 3")
print("å‰Šé™¤å®Œäº†")
```

### Step 17: å‰Šé™¤å¾Œã®ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª

```python
table.refresh()
df = table.scan().to_pandas()
print(df)
```

```
   user_id   name                email
0        1  Alice    alice@example.com
1        2    Bob  bob.new@example.com
2        4  David    david@example.com
3        5    Eve      eve@example.com
```

CharlieãŒå‰Šé™¤ã•ã‚Œã¦ã„ã¾ã™ã€‚

```python
for snap in table.metadata.snapshots:
    print(f"Snapshot ID: {snap.snapshot_id}")
    print(f"  Operation: {snap.summary.get('operation')}")
    print(f"  Added files: {snap.summary.get('added-data-files', 0)}")
    print(f"  Deleted files: {snap.summary.get('deleted-data-files', 0)}")
    print(f"  Deleted records: {snap.summary.get('deleted-records', 0)}")
    print()
```

```
Snapshot ID: 5815774430499421792
  Operation: Operation.APPEND
  Added files: 1
  Deleted files: None
  Deleted records: None

Snapshot ID: 3879896634347338753
  Operation: Operation.APPEND
  Added files: 1
  Deleted files: None
  Deleted records: None

Snapshot ID: 5929255525955454062
  Operation: Operation.OVERWRITE
  Added files: 1
  Deleted files: 1
  Deleted records: 3

Snapshot ID: 7234342154326247462
  Operation: Operation.APPEND
  Added files: 1
  Deleted files: None
  Deleted records: None

Snapshot ID: 4494934084334135847
  Operation: Operation.OVERWRITE
  Added files: 1
  Deleted files: 1
  Deleted records: 2
```

å‰Šé™¤æ“ä½œã¯`Operation.OVERWRITE`ã¨ã—ã¦è¨˜éŒ²ã•ã‚Œã€`deleted-records`ã¨`deleted-data-files`ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚Icebergã§ã¯å‰Šé™¤ã‚‚å†…éƒ¨çš„ã«ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãæ›ãˆï¼ˆCopy-on-Writeï¼‰ã¨ã—ã¦å‡¦ç†ã•ã‚Œã¾ã™ã€‚

3ç•ªç›®ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆ`Deleted records: 3`ï¼‰ã¯Step 14ã§ã®æ›´æ–°æ™‚ã€5ç•ªç›®ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆ`Deleted records: 2`ï¼‰ã¯ä»Šå›ã®å‰Šé™¤æ™‚ã®ã‚‚ã®ã§ã™ã€‚

:::message
æ›´æ–°æ™‚ã¨åŒæ§˜ã«ã€`Deleted records` ã¯ã€Œãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã§è«–ç†å‰Šé™¤ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã€ã§ã™ã€‚Charlieã ã‘ã‚’å‰Šé™¤ã—ã¦ã‚‚ã€åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã¾ã‚Œã¦ã„ãŸä»–ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚‚è«–ç†å‰Šé™¤ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆã•ã‚Œã¾ã™ã€‚å‰Šé™¤å¯¾è±¡ä»¥å¤–ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã«å†æ›¸ãè¾¼ã¿ã•ã‚Œã¾ã™ã€‚
:::

## ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«

Icebergã®å¤§ããªç‰¹å¾´ã®ä¸€ã¤ãŒã€éå»ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã€Œã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«ã€æ©Ÿèƒ½ã§ã™ã€‚

### Step 18: éå»ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã‚€

```python
# æœ€åˆã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®IDã‚’å–å¾—
first_snapshot_id = table.metadata.snapshots[0].snapshot_id

# éå»ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã§ã‚¹ã‚­ãƒ£ãƒ³
old_df = table.scan(snapshot_id=first_snapshot_id).to_pandas()
print("=== æœ€åˆã®appendæ™‚ç‚¹ã®ãƒ‡ãƒ¼ã‚¿ ===")
print(old_df)
```

```
=== æœ€åˆã®appendæ™‚ç‚¹ã®ãƒ‡ãƒ¼ã‚¿ ===
   user_id     name              email
0        1    Alice  alice@example.com
1        2      Bob    bob@example.com
2        3  Charlie               None
```

æ›´æ–°å‰ã®Bobã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚„ã€å‰Šé™¤ã—ãŸCharlieã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒç¢ºèªã§ãã¾ã™ã€‚

## ã¾ã¨ã‚

ã“ã®è¨˜äº‹ã§ã¯ã€uv + PyIcebergã‚’ä½¿ã£ã¦Apache Icebergã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚

### å­¦ã‚“ã ã“ã¨

1. **ã‚«ã‚¿ãƒ­ã‚°**: ãƒ†ãƒ¼ãƒ–ãƒ«ã®å ´æ‰€ã‚’ç®¡ç†ï¼ˆä»Šå›ã¯SQLiteã€æœ¬ç•ªã§ã¯Glue Data Catalogãªã©ï¼‰
2. **metadata.json**: ãƒ†ãƒ¼ãƒ–ãƒ«ã®çŠ¶æ…‹ã‚’è¨˜éŒ²ã€‚æ“ä½œã”ã¨ã«æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã‚‹
3. **ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒªã‚¹ãƒˆ/ãƒ•ã‚¡ã‚¤ãƒ«**: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ã¨çµ±è¨ˆæƒ…å ±ã‚’éšå±¤çš„ã«ç®¡ç†
4. **ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ**: å„æ™‚ç‚¹ã®ãƒ†ãƒ¼ãƒ–ãƒ«çŠ¶æ…‹ã€‚ã‚¿ã‚¤ãƒ ãƒˆãƒ©ãƒ™ãƒ«ã®åŸºç›¤
5. **Copy-on-Write**: æ›´æ–°ãƒ»å‰Šé™¤æ™‚ã¯å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ®‹ã—ã¦æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

- **AWS S3 Tables**: Icebergã‚’ãƒãƒãƒ¼ã‚¸ãƒ‰ã§ä½¿ãˆã‚‹AWSã‚µãƒ¼ãƒ“ã‚¹
- **dbt-athena**: dbtã‹ã‚‰AthenaçµŒç”±ã§Icebergãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ“ä½œ
- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°**: Hidden Partitioningã«ã‚ˆã‚‹æŸ”è»Ÿãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ç®¡ç†

Icebergã®å†…éƒ¨æ§‹é€ ã‚’ç†è§£ã—ã¦ãŠãã¨ã€S3 Tablesã‚„Athenaã‚’ä½¿ã†éš›ã«ã‚‚ã€Œè£ã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã‹ã€ãŒã‚ã‹ã‚Šã€ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚„è¨­è¨ˆåˆ¤æ–­ã«å½¹ç«‹ã¤ã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã™ã€‚

## å‚è€ƒè³‡æ–™

- [Apache Icebergå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://iceberg.apache.org/docs/latest/)
- [PyIcebergå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://py.iceberg.apache.org/)
- [uvå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.astral.sh/uv/)
